{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "436ae7d0-27b5-4e56-87fa-497c587a3b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd5e5371-1558-436b-ad41-e70de49c11aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load  the dataset\n",
    "(X_train, y_train),(X_test, y_test)=mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75bc6358-d58c-4144-a6a0-762d1a392431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (60000, 28, 28)\n",
      "y_train shape (60000,)\n",
      "X_test shape (10000, 28, 28)\n",
      "X_test shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "#let us print the shape of the datasets\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"X_test shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8361e8f8-a124-4dc4-9878-f656a4887b06",
   "metadata": {},
   "source": [
    "###### Steps to build an Image Classification Model using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dba68f4-2ef3-4160-9023-13def5091820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Flatten the input image dimension to 1D (width pixels X height)\n",
    "# 2. Normalise the image pixel values (divided by 255)\n",
    "# 3. One hot encode the categorical column\n",
    "# 4. Build a model architecture (Sequential) with dense Layers(fully Connected layers)\n",
    "# 5. Train the model and make the predictiond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3ab1650-d909-44a0-920f-4c62f604d9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#The np_utils is used for categorical encodings, such as converting class vectors to binary class matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b55c7b27-fa15-4425-81ed-a0a302fc2778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_class = [2, 5, 7, 1]\n",
    "y_binary = to_categorical(y_class)\n",
    "print(y_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f583f13-c7de-4da4-ad06-eb9d0a206490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flattering the image from the 28x28 pixels to 1D 787 pixels.\n",
    "X_train=X_train.reshape(60000,784)\n",
    "X_test=X_test.reshape(10000,784)\n",
    "X_train=X_train.astype('float32')\n",
    "X_test=X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84e314a5-4105-4c37-8202-b3abd2795dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalising the data to help with the training\n",
    "X_train=X_train/255 #we can write like this as well X_train /=255\n",
    "X_test=X_test/255 #we can write like this as well X_test /=255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3723353-a13c-4319-9b83-e91c86a6f0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before one hot encoding:  (60000,)\n",
      "Shape after one hot encoding:  (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "#one hot encoding using keras numpy related utilities\n",
    "n_classes=10\n",
    "print(\"Shape before one hot encoding: \", y_train.shape)\n",
    "y_train=to_categorical(y_train,n_classes)\n",
    "y_test=to_categorical(y_test,n_classes)\n",
    "print(\"Shape after one hot encoding: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62cea0d5-ced7-428d-8b18-b6ac8eddcd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building a linear stock with sequential model\n",
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eede74d3-678c-489d-98b1-322ad7f3d612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python 3.11\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#hidden layer\n",
    "model.add(Dense(100,input_shape=(784,), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23f052a1-6399-49c9-af8a-0a8c3420d589",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output layer\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e163a76-ae14-483f-9fcb-ad8f2a264245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">78,500</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m78,500\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │           \u001b[38;5;34m1,010\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">79,510</span> (310.59 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m79,510\u001b[0m (310.59 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">79,510</span> (310.59 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m79,510\u001b[0m (310.59 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#looking the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2489a53b-d77f-41d4-9893-ee52b841b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiling the sequential model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d6246d8-c947-4722-9766-83dc09b61a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.8207 - loss: 0.6505 - val_accuracy: 0.9421 - val_loss: 0.2021\n",
      "Epoch 2/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9472 - loss: 0.1826 - val_accuracy: 0.9575 - val_loss: 0.1465\n",
      "Epoch 3/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9626 - loss: 0.1339 - val_accuracy: 0.9625 - val_loss: 0.1196\n",
      "Epoch 4/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9714 - loss: 0.1040 - val_accuracy: 0.9684 - val_loss: 0.1034\n",
      "Epoch 5/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9769 - loss: 0.0827 - val_accuracy: 0.9680 - val_loss: 0.1040\n",
      "Epoch 6/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9803 - loss: 0.0705 - val_accuracy: 0.9737 - val_loss: 0.0863\n",
      "Epoch 7/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9826 - loss: 0.0602 - val_accuracy: 0.9746 - val_loss: 0.0809\n",
      "Epoch 8/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9857 - loss: 0.0512 - val_accuracy: 0.9741 - val_loss: 0.0861\n",
      "Epoch 9/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9883 - loss: 0.0428 - val_accuracy: 0.9755 - val_loss: 0.0828\n",
      "Epoch 10/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9894 - loss: 0.0395 - val_accuracy: 0.9765 - val_loss: 0.0788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1d75ef36250>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training the model for 10 epochs\n",
    "model.fit(X_train,y_train, batch_size=128, epochs=10,\n",
    "         validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08baa87e-58ca-4489-a5aa-b466f49565e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
